{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "from sklearn import svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from dtreeviz.trees import dtreeviz\n",
    "import csv\n",
    "import inflect\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from six import StringIO  \n",
    "from sklearn import tree\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import scipy.interpolate as inter\n",
    "import matplotlib.image as mpimg\n",
    "import skimage\n",
    "from skimage import feature\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definitions for decision tree analysis\n",
    "\n",
    "def getNumberRows(csv_file_path):\n",
    "  '''\n",
    "  returns the number of rows of data in a csv file.\n",
    "\n",
    "  @param csv_file_path : path to the CSV you wish to read\n",
    "\n",
    "  @return number of rows of data\n",
    "  '''\n",
    "  with open(csv_file_path, 'r') as path:\n",
    "\t    \n",
    "    reader = csv.reader(path)\n",
    "    print(\"Calculating number of rows in file...\")\n",
    "    row_count = sum(1 for row in reader)\n",
    "    print(\"Found \" + str(row_count) + \" rows...\")\n",
    "    return row_count\n",
    "\n",
    "def read_data(csv_file_path):\n",
    "  '''\n",
    "  read data from csv file and returns data as a tuple comtaining feature names and data array.\n",
    "\n",
    "  @param csv_file_path : path to the CSV you wish to read\n",
    "\n",
    "  >>> data = read_data(\"/Users/jackal/Documents/CODEX/OCO2_lnd_glint_subsample_1M.csv\")\n",
    "  Calculating number of rows in file...\n",
    "  Found 1000001 rows...\n",
    "  >>> print(data.shape)\n",
    "  (1000000, 122)\n",
    "  @return a tuple containing feature names and data array\n",
    "  '''\n",
    "  with open(csv_file_path, 'r') as path:\n",
    "\t    \n",
    "    reader = csv.reader(path)\n",
    "    count = 0\n",
    "    row_count = getNumberRows(csv_file_path)\n",
    "\n",
    "    for line in reader:\n",
    "\n",
    "      if(count == 0):\n",
    "        featureNames = line\n",
    "        numFeatures = len(featureNames)\n",
    "        data = np.ones((row_count -1, numFeatures), dtype=float)\n",
    "        count += 1\n",
    "\n",
    "      else:\n",
    "\n",
    "        try:\n",
    "          try:\n",
    "            data[count-1,:] = [float(x) for x in line]\n",
    "            count += 1\n",
    "        \n",
    "          except:\n",
    "            nums = line[numFeatures-1].split('.')\n",
    "            front = nums[0] + \".\" + nums[1][0:-1]\n",
    "            line[numFeatures-1] = front\n",
    "            data[count - 1, :] = [float(x) for x in line[0:numFeatures]]\n",
    "            break\n",
    "          \n",
    "          \n",
    "\n",
    "        except:\n",
    "          print(f\"Warning: error reading csv line {count + 1}\")\n",
    "          break\n",
    "    \n",
    "    print(f\"Found {count} unique data entries...\")\n",
    "    return (featureNames, data[0:count])\n",
    "\n",
    "def find_feature_index(name, featureNames):\n",
    "  '''\n",
    "  returns the index of the feature 'name' in the list featureName.\n",
    "\n",
    "  @param name : the name of the feature to look for\n",
    "  @param featureNames : the list of featureNames to look through\n",
    "\n",
    "  >>> f_index = find_feature_name(retrieval_latitude', featureNames)\n",
    "  Found retrieval_latitude at index 24\n",
    "\n",
    "  @return the index of the searched feature.\n",
    "  '''\n",
    "  for x in range(0,len(featureNames)):\n",
    "    if(name in featureNames[x]):\n",
    "      print(\"Found \" + name + \" at index \" + str(x))\n",
    "      return x\n",
    "\n",
    "  return None\n",
    "\n",
    "def downselect_data(data, labels, latIndex, longIndex, latRange, longRange):\n",
    "  '''\n",
    "  removes data entries from data according to the ranges in latRange and longRange.\n",
    "\n",
    "  @param data : the name to alter\n",
    "  @param labels : the list of labels for data\n",
    "  @param latIndex : the index of latitude feature\n",
    "  @param longIndex : the index of longitude feature\n",
    "  @param larRange : a list of length two with the lower and upper bound of latitude\n",
    "  @param longrange : a list of length two with the lower and upper bound of longitude\n",
    "\n",
    "  >>> data = downselect_data(data, labels, latIndex, longIndex, (33,80), (30,120))\n",
    "  472 of 585 indices in latitude and longitude range. Using 81 % of data\n",
    "  \n",
    "\n",
    "  @return the downselected data\n",
    "  '''\n",
    "  num_data = len(labels)\n",
    "  indices_in_bounds = ((data[:,latIndex] > latRange[0]) & (data[:,latIndex]< latRange[1])) & ((data[:,longIndex] >longRange[0]) & (data[:,longIndex] < longRange[1]))\n",
    "  data_in_bounds = data[indices_in_bounds]\n",
    "  labels_in_bounds = labels[indices_in_bounds]\n",
    "  num_data_in_bounds = len(labels_in_bounds)\n",
    "  print(num_data_in_bounds, \"of\", num_data, \"indices in latitude and longitude range. Using\", round(100*num_data_in_bounds/num_data,2), \"% of data\")\n",
    "  return data_in_bounds, labels_in_bounds\n",
    "\n",
    "def run_decision_tree(X, Y, search_hyperparams = True):\n",
    "  '''\n",
    "  trains a decision tree on the data X and target list Y. Saves the decision tree as decision_tree.svg\n",
    "  May search through hyperparameters best_min_split and best_min_impurity_decrease\n",
    "\n",
    "  @param X : the feature data\n",
    "  @param Y : the target data\n",
    "  @param search_hyperparams : =True means the program will use cross-validation to limit tree growth\n",
    "\n",
    "  >>> run_decision_tree(X,Y)\n",
    "  Training Decision Tree...\n",
    "  Searching hyperparameters to trim tree...\n",
    "  Best minimum impurity decrease is 0.02 with a cross-validation accuracy of 92.58 %\n",
    "  Best minimum samples split is 26 with a cross-validation accuracy of 92.37 %\n",
    "\n",
    "  @return the decision tree and a tuple with the best_min_split and best_min_impurity_decrease\n",
    "  '''\n",
    "  print(\"Training Decision Tree...\")\n",
    "  random_state = np.random.RandomState(0)\n",
    "  X = np.nan_to_num(X).astype(np.float32)\n",
    "  Y = np.nan_to_num(Y).astype(np.float32)\n",
    "  num_data = len(Y)\n",
    "\n",
    "  if search_hyperparams:\n",
    "    print(\"Searching hyperparameters to trim tree...\")\n",
    "\n",
    "    # Use a K fold cross-validtion (k=10) to choose certain hyper parameters of the decision tree\n",
    "    # This is done to strain out splits in the decision tree that do not generalize to the underlying distribution\n",
    "    skf = StratifiedKFold(n_splits = 10, random_state = random_state, shuffle = True)\n",
    "\n",
    "    # We search through possible values of min_impurity_decrease. This prohibits a node from spliting unless the split causes a an impurity decrease\n",
    "    # above a certain threshold. The values range from 0 to 0.2.\n",
    "    min_impurity_decrease_range = np.linspace(0,.2,11)\n",
    "    impurity_scores = np.zeros(11)\n",
    "\n",
    "    # We also search through different values of min_samples_split. This prohibits a node from splitting if there are too few samples at that node.\n",
    "    # values range from 2 to a quarter of the data.\n",
    "    min_num_to_split_range = range(2,int(num_data/4),int(num_data/100))\n",
    "    min_split_scores = np.zeros(len(min_num_to_split_range))\n",
    "\n",
    "    # Train a decision tree for each of the k folds, searching through min_impurity_decrease and min_samples_split separately.\n",
    "    for train_index, test_index in skf.split(X,Y):\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "      for i, min_impurity_decrease in enumerate(min_impurity_decrease_range):\n",
    "        clf_skf = tree.DecisionTreeClassifier(min_impurity_decrease=min_impurity_decrease, random_state=random_state)\n",
    "        clf_skf = clf_skf.fit(X_train, Y_train)\n",
    "        impurity_scores[i] += sum(clf_skf.predict(X_test) == Y_test)\n",
    "      for i, min_num_to_split in enumerate(min_num_to_split_range):\n",
    "        clf_skf = tree.DecisionTreeClassifier(min_samples_split = min_num_to_split, random_state=random_state)\n",
    "        clf_skf = clf_skf.fit(X_train, Y_train)\n",
    "        min_split_scores[i] += sum(clf_skf.predict(X_test) == Y_test)\n",
    "    \n",
    "    # Find the decision tree with best cross-validaton accuracy, find it's accuracy and the value of min_impurity_decrease, and report it.\n",
    "    best_impurity_index = np.argmax(impurity_scores)\n",
    "    best_min_impurity_decrease = min_impurity_decrease_range[best_impurity_index]\n",
    "    best_impurity_score = impurity_scores[best_impurity_index]\n",
    "    print(\"Best minimum impurity decrease is\", best_min_impurity_decrease, \"with a cross-validation accuracy of \", round(100*best_impurity_score/num_data,2), \"%\")\n",
    "\n",
    "    # Find the decision tree with best cross-validaton accuracy, find it's accuracy and the value of min_samples_split, and report it.\n",
    "    best_min_split_index = np.argmax(min_split_scores)\n",
    "    best_min_split = min_num_to_split_range[best_min_split_index]\n",
    "    best_min_split_score = min_split_scores[best_min_split_index]\n",
    "    print(\"Best minimum samples split is\", best_min_split, \"with a cross-validation accuracy of\", round(100*best_min_split_score/num_data,2), \"%\")\n",
    "  else:\n",
    "    best_min_split = 2\n",
    "    best_min_impurity_decrease = 0.0\n",
    "\n",
    "  # train a decision tree, using the best min_impurity_decrease and min_samples_split values if searched for, and report it.\n",
    "  clf = tree.DecisionTreeClassifier(min_samples_split=best_min_split, min_impurity_decrease=best_min_impurity_decrease,random_state=random_state)\n",
    "  clf = clf.fit(X, Y)\n",
    "  return clf, (best_min_split,best_min_impurity_decrease)\n",
    "\n",
    "def plot_decision_tree(model, data, labels, feature_names, ln, plot_name):\n",
    "  '''\n",
    "  plots the decision tree\n",
    "\n",
    "  @param model : the decision tree\n",
    "  @param data : feture data used to train model\n",
    "  @param labels : target data used to train model\n",
    "  @param feature_names : list of feature names\n",
    "  @param ln : name of target\n",
    "  @param plot_name : title of the plot\n",
    "  ''' \n",
    "    \n",
    "  class_names = list(set(labels))\n",
    "  p = inflect.engine()\n",
    "  class_names = [p.number_to_words(int(x)) for x in class_names]\n",
    "  dot_data = tree.export_graphviz(model, feature_names=feature_names, class_names = class_names, filled=True, rounded=True, special_characters=True, out_file=None) \n",
    "  graph = graphviz.Source(dot_data)\n",
    "  graph.render(plot_name)\n",
    "\n",
    "  viz = dtreeviz(model, data, labels,\n",
    "          target_name=ln[0],\n",
    "          feature_names=feature_names,\n",
    "          class_names= class_names)\n",
    "\n",
    "  viz.save(\"decision_tree.svg\")\n",
    "  viz\n",
    "\n",
    "def plot_decision_area(model, X, Y, pair_of_feature_names, feature_names, label_names):\n",
    "  '''\n",
    "  plots the decision area of a decision tree that splits for only two features\n",
    "\n",
    "  @param model : the decision tree\n",
    "  @param X : feture data used to train model\n",
    "  @param Y : target data used to train model\n",
    "  @param pair_of_feature_names : list of two feature names to plot\n",
    "  @param feauture_names : list of all feature names\n",
    "  @param plot_name : name of target\n",
    "  '''\n",
    "\n",
    "  feature_1_name, feature_2_name = (feature_names.index(pair_of_feature_names[0]), feature_names.index(pair_of_feature_names[1]))\n",
    "  feature_indices = (feature_names.index(pair_of_feature_names[0]),feature_names.index(pair_of_feature_names[1]))\n",
    "  X_alt = X[:,feature_indices]\n",
    "  feature_1 = np.linspace(X_alt[:, 0].min(), X_alt[:, 0].max())\n",
    "  feature_2 = np.linspace(X_alt[:, 1].min(), X_alt[:, 1].max())\n",
    "  input = np.zeros(len(feature_names))\n",
    "  Y_pred = np.zeros((len(feature_1),len(feature_2)))\n",
    "  for i, f1 in enumerate(feature_1):\n",
    "    for j, f2 in enumerate(feature_2):\n",
    "      input[feature_indices[0]] = f1\n",
    "      input[feature_indices[1]] = f2\n",
    "      Y_pred[j,i] = model.predict(input.reshape(1,-1))\n",
    "  display = DecisionBoundaryDisplay(\n",
    "    xx0 = feature_1, xx1 = feature_2, response = Y_pred\n",
    "  )\n",
    "  display.plot()\n",
    "  display.ax_.scatter(\n",
    "    X_alt[:,0],X_alt[:,1], c=Y, edgecolor=\"black\",\n",
    "  )\n",
    "  display.ax_.set_xlabel(pair_of_feature_names[0])\n",
    "  display.ax_.set_ylabel(pair_of_feature_names[1])\n",
    "  display.ax_.set_title(\"Decision Boundary for \" + pair_of_feature_names[0] + \" vs \" + pair_of_feature_names[1])\n",
    "  plt.show()\n",
    "\n",
    "def analyze_decision_tree(X,Y,clf,hyper_params,feature_names):\n",
    "  '''\n",
    "  pesents data on the decision tree.\n",
    "  presents model accuracy and the test accuracy when a decision tree with similar hyperparameters is run through 10-fold cross validation\n",
    "  presents feature importances for splitting features of decision tree\n",
    "  presents differences in feature importance between the decision tree and an untrimmed tree trained on the same data\n",
    "\n",
    "  @param X : the feature data\n",
    "  @param Y : the target data\n",
    "  @param clf : the decision tree\n",
    "  @param hyper_params : a list of the values of best_min_split and best_min_impurity_decrease used to train the decision tree\n",
    "  @param feature_names : a list of the feature names\n",
    "  '''\n",
    "\n",
    "  print(\"Comparing model to untrimmed tree...\")\n",
    "  random_state = np.random.RandomState(0)\n",
    "  X = np.nan_to_num(X).astype(np.float32)\n",
    "  Y = np.nan_to_num(Y).astype(np.float32)\n",
    "  num_data = len(Y)\n",
    "\n",
    "  # Get accuracy of model\n",
    "  model_accuracy = round(100*clf.score(X,Y),2)\n",
    "  \n",
    "  # To validate that the model generalizes, we compare the accuracy of our model to cross-validated accuracy of models trained with same hyper-parameters.\n",
    "  skf = StratifiedKFold(n_splits = 10, random_state = random_state, shuffle = True)\n",
    "  score = 0\n",
    "  for train_index, test_index in skf.split(X,Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    val_clf = tree.DecisionTreeClassifier(min_samples_split= hyper_params[0],  min_impurity_decrease = hyper_params[1], random_state = random_state)\n",
    "    val_clf = val_clf.fit(X_train, Y_train)\n",
    "    score += sum(val_clf.predict(X_test) == Y_test)\n",
    "  val_accuracy = round(100*score/num_data,2)\n",
    "  print(\"model accuracy:\", model_accuracy, \"%\")\n",
    "  print(\"unpruned tree accuracy (10-fold cross-validation):\", val_accuracy, \"%\")\n",
    "\n",
    "  # Report the importance of each feature with non-zero importance\n",
    "  import_list = []\n",
    "  for importance, label in sorted(zip(clf.feature_importances_,feature_names),reverse = True):\n",
    "    if importance != 0:\n",
    "      import_list.append(label+ \": \"+ str(round(importance,3)))\n",
    "  print()\n",
    "  print(\"feature: importance\")\n",
    "  print(\"-------------------\")\n",
    "  print(import_list)\n",
    "  \n",
    "  # Further validate the importances by training a full tree and comparing importances\n",
    "\n",
    "  clf_full = tree.DecisionTreeClassifier(random_state = random_state)\n",
    "  clf_full.fit(X,Y)\n",
    "  import_list_comparison = []\n",
    "  importance_diffs = [ x-y for (x,y) in zip(clf.feature_importances_, clf_full.feature_importances_)]\n",
    "  for importance_diff, label in sorted(zip(importance_diffs,feature_names),reverse = True):\n",
    "    import_list_comparison.append(label + \": \" + str(round(importance_diff,3)))\n",
    "  print()\n",
    "  print(\"comparing importances to a full tree (pruned model importance - full model importance):\")\n",
    "  print(\"-------------------\")\n",
    "  print(import_list_comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main block for decision tree analysis\n",
    "\n",
    "# stipulate latitude and longitude ranges\n",
    "latRange = (33,80)\n",
    "longRange = (30,120)\n",
    "\n",
    "# read csv files\n",
    "fn,data = read_data(\"7500_data.csv\")\n",
    "ln,labels = read_data(\"7500_labels.csv\")\n",
    "\n",
    "# retrieve latitude and logitude indices\n",
    "latIndex = find_feature_index(\"retrieval_latitude\", fn)\n",
    "longIndex = find_feature_index(\"retrieval_longitude\", fn)\n",
    "labels = labels.flatten()\n",
    "\n",
    "# downselect data based on acceptable ranges\n",
    "data,labels = downselect_data(data, labels, latIndex, longIndex,latRange, longRange)\n",
    "\n",
    "# run decision tree\n",
    "clf, hyper_params = run_decision_tree(data,labels,search_hyperparams=True)\n",
    "\n",
    "# plot decision tree. check folder for image of decision tree\n",
    "plot_decision_tree(clf, data, labels, fn, ln, \"decision_tree\")\n",
    "\n",
    "# analyze decision tree\n",
    "analyze_decision_tree(data,labels,clf,hyper_params,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that our decision tree makes splits on only two features,, it would be informative to give a plot of the decision area \n",
    "# on the two separating features: retrieval_altitude and surface_pressure_delta_abp.\n",
    "\n",
    "plot_decision_area(clf, data, labels, ('retrieval_altitude', 'surface_pressure_delta_abp'), feature_names = fn, label_names=list(set(labels)), )\n",
    "\n",
    "# in the unpruned decision tree, retrieval_latitude had 0.05 importance, which makes it about as important as surface_pressure_delta_abp in our untrimmed model. However,\n",
    "# our validation indicated that spliting only on surface_pressure_delta_abp generalized the data better when it was kept from growing larger and incorporating retrieval_latitude.\n",
    "# this suggests that perhaps retrieval_latitude tracks some of the outliers seen in the graph below."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
